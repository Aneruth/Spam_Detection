{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "from posixpath import abspath\n",
    "import sys,os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "sys.path.insert(0,os.path.join('Code/Preprocess'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tensorflow as tf # Adding padding to our dataset (train and test models)\n",
    "import numpy as np\n",
    "from DataPreparation import dataPrepare\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "\n",
    "    def __init__(self,path) -> None:\n",
    "        self.path = path\n",
    "\n",
    "    def getData(self):\n",
    "        \"\"\"Considers the path of the dataset which holds the sparse matrix. With the help of tensorflow we are\n",
    "        creating the padding for our dataset (X_train and X_test).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Tensor data with padding and dataframe\n",
    "        \"\"\"\n",
    "        fetchData = dataPrepare()\n",
    "    \n",
    "        X_train, X_test, y_train, y_test =  fetchData.Vectorizer(self.path)\n",
    "\n",
    "        # Adding the padding to our sparse matrix\n",
    "        X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train.todense())\n",
    "        X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test.todense())\n",
    "\n",
    "        # make training and test sets in torch\n",
    "        x_train = torch.from_numpy(X_train).type(torch.Tensor)\n",
    "        x_test = torch.from_numpy(X_test).type(torch.Tensor)\n",
    "        y_train = torch.from_numpy(y_train.to_numpy()).type(torch.Tensor)\n",
    "        y_test = torch.from_numpy(y_test.to_numpy()).type(torch.Tensor)\n",
    "        \n",
    "        print(f'Size of our y-train dataset is {y_train.size()} and X-train dataset is {x_train.size()}')\n",
    "\n",
    "        return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"LSTM model which takes the following input:\n",
    "                - Input Dimension\n",
    "                - Hidden Layers (Number)\n",
    "                - Total Number of Layers\n",
    "                - Ouput Diemension\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Length of the dataset\n",
    "            hidden_dim (int): Total hidden layer\n",
    "            num_layers (int): Total layers to be present\n",
    "            output_dim (int): Number of output diemension layer\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # https://discuss.pytorch.org/t/why-do-i-get-typeerror-expected-np-ndarray-got-numpy-ndarray-when-i-use-torch-from-numpy-function/37525/3\n",
    "        x = torch.from_numpy(np.asarray(x)).type(torch.Tensor)\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers,tuple(x.size())[0] , self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, tuple(x.size())[0], self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset object to support batch training\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features             \n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMImplement:\n",
    "\n",
    "    def __init__(self,path, hidden_dim, num_layers, output_dim) -> None:\n",
    "        \"\"\"A cosntructor which takes the path of the dataset, number of hidden dimensions, number of layers and output\n",
    "        diemensions as our parameters. This will plot our accuracy plot and loss plot.\n",
    "\n",
    "        Args:\n",
    "            path (String): Path of our dataset\n",
    "            hidden_dim (int): Total hidden layer\n",
    "            num_layers (int): Total layers to be present\n",
    "            output_dim (int): Number of output diemension layer\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def run(self):\n",
    "        from torch.utils.data import DataLoader\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        fetchData = dataPrepare()\n",
    "        X_train, X_test, y_train, y_test =  fetchData.Vectorizer(self.path)\n",
    "        x_train,x_test,y_train,y_test = Data(self.path).getData()\n",
    "        x_val,_,y_val,_ = train_test_split(x_train,y_train,random_state=100,test_size=0.5)\n",
    "        input_dimension = X_train.shape[1]\n",
    "        model = LSTM(input_dim=input_dimension, hidden_dim=self.hidden_dim, output_dim=self.output_dim, num_layers=self.num_layers)\n",
    "\n",
    "        # https://discuss.pytorch.org/t/valueerror-target-size-torch-size-16-must-be-the-same-as-input-size-torch-size-16-1/55232\n",
    "        y_train = y_train.unsqueeze(1)\n",
    "        y_train = y_train.float()\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        train_dataset = Dataset(x_train, y_train)\n",
    "        dataloader = DataLoader(train_dataset,batch_size=16, shuffle=True)\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        print(len(list(model.parameters())))\n",
    "        for i in range(len(list(model.parameters()))):\n",
    "            print(list(model.parameters())[i].size())\n",
    "\n",
    "        num_epochs = 100\n",
    "        loss_val  = np.zeros(num_epochs)\n",
    "        acc_val  = np.zeros(num_epochs)\n",
    "        \n",
    "        for t in range(num_epochs):\n",
    "            model.train()\n",
    "            for idx, (X_batch, Y_batch) in enumerate(dataloader):\n",
    "                # Forward pass\n",
    "                output = model(X_batch)  # conduct forward pass\n",
    "\n",
    "                loss = loss_fn(output, Y_batch)\n",
    "                loss += 0.001 * sum(p.abs().sum() for p in model.parameters())\n",
    "                # if t % 10 == 0 and t !=0:\n",
    "                #     print(f\"Epoch {t} MSE is {loss.item()}\")\n",
    "                loss_val[t] = loss.item()\n",
    "\n",
    "                # Zero out gradient, else they will accumulate between epochs\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                optimiser.step()\n",
    "            \n",
    "            with torch.no_grad():  # no need to calculate gradients when assessing accuracy\n",
    "\n",
    "                model.eval()\n",
    "                pred_train = model(x_train).numpy().argmax(axis=1)\n",
    "                train_acc = (pred_train == y_train.numpy()).mean()\n",
    "                pred_val = model(x_val).numpy().argmax(axis=1)\n",
    "                val_acc = (pred_val == y_val.numpy()).mean()\n",
    "                acc_val[t] = val_acc\n",
    "        return acc_val,loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our y-train dataset is torch.Size([952]) and X-train dataset is torch.Size([952, 1, 2701])\n",
      "6\n",
      "torch.Size([128, 2701])\n",
      "torch.Size([128, 32])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([16, 1])) must be the same as input size (torch.Size([16, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/72/b37ygcg91x370txz29wprwjc0000gn/T/ipykernel_14053/1560802266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMImplement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'YoutubeComplete.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/72/b37ygcg91x370txz29wprwjc0000gn/T/ipykernel_14053/2237144916.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# conduct forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m# if t % 10 == 0 and t !=0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/Spam_Detection/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/Spam_Detection/venv/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    705\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/Spam_Detection/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([16, 1])) must be the same as input size (torch.Size([16, 2]))"
     ]
    }
   ],
   "source": [
    "acc,loss = LSTMImplement('YoutubeComplete.csv',32,1,2).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(acc, label=\"Model accuracy\")\n",
    "plt.legend()\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.14285714285714"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acc_val.unique()\n",
    "np.unique(acc_val)[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.72268907563025"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "    from torch.utils.data import DataLoader\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    fetchData = dataPrepare()\n",
    "    X_train, X_test, y_train, y_test =  fetchData.Vectorizer(self.path)\n",
    "    x_train,x_test,y_train,y_test = Data(self.path).getData()\n",
    "    x_val,_,y_val,_ = train_test_split(x_train,y_train,random_state=100,test_size=0.5)\n",
    "    input_dimension = X_train.shape[1]\n",
    "    model = LSTM(input_dim=input_dimension, hidden_dim=self.hidden_dim, output_dim=self.output_dim, num_layers=self.num_layers)\n",
    "\n",
    "    # https://discuss.pytorch.org/t/valueerror-target-size-torch-size-16-must-be-the-same-as-input-size-torch-size-16-1/55232\n",
    "    y_train = y_train.unsqueeze(1)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataset = Dataset(x_train, y_train)\n",
    "    dataloader = DataLoader(train_dataset,batch_size=16, shuffle=True)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    print(len(list(model.parameters())))\n",
    "    for i in range(len(list(model.parameters()))):\n",
    "        print(list(model.parameters())[i].size())\n",
    "\n",
    "    num_epochs = 100\n",
    "    loss_val  = np.zeros(num_epochs)\n",
    "    acc_val  = np.zeros(num_epochs)\n",
    "    correct_values = 0\n",
    "    for epochs in range(num_epochs):\n",
    "        model.train()\n",
    "        for idx, (inputs,labels) in enumerate(dataloader):\n",
    "            # Forward pass\n",
    "            output = model(inputs)  # conduct forward pass\n",
    "\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss += 0.001 * sum(p.abs().sum() for p in model.parameters())\n",
    "            # if t % 10 == 0 and t !=0:\n",
    "            #     print(f\"Epoch {t} MSE is {loss.item()}\")\n",
    "            loss_val[epochs] = loss.item()\n",
    "\n",
    "            # Zero out gradient, else they will accumulate between epochs\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimiser.step()\n",
    "\n",
    "            correct_values += (output == labels).float().sum()\n",
    "            print(correct_values)\n",
    "        # with torch.no_grad():  # no need to calculate gradients when assessing accuracy\n",
    "\n",
    "        # model.eval()\n",
    "        # accuracy = 100 * correct_values / len(y_train.numpy())\n",
    "        # avg_loss = sum(loss) / len(y_train.numpy())\n",
    "        # print(f\"Accuracy is {accuracy} and avg loss is {avg_loss.item()}\")\n",
    "        # pred_train = model(x_train).detach().numpy().argmax(axis=1)\n",
    "        # train_acc = (pred_train == y_train.detach().numpy()).mean()\n",
    "        # pred_val = model(x_val).detach().numpy().argmax(axis=1)\n",
    "        # val_acc = (pred_val == y_val.detach().numpy()).mean()\n",
    "        # acc_val[epochs] = val_acc\n",
    "    return acc_val,loss_val"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62b984ecf3fa21db91abb9ce3361d5af483ec95077de7660821dc35f86aea7e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
